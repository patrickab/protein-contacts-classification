{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiAHkyDdV8-t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "from biopandas.pdb import PandasPdb\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from Bio.PDB import DSSP, HSExposureCB, PPBuilder, is_aa, NeighborSearch\n",
        "from Bio.PDB.MMCIFParser import MMCIFParser\n",
        "from Bio.SeqUtils import seq1\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, CategoricalNB, ComplementNB, BernoulliNB\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from timeit import default_timer as timer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import matthews_corrcoef, balanced_accuracy_score, average_precision_score, roc_auc_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWqgNFSup2mP"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "current_directory = os.getcwd()\n",
        "path_ring = current_directory + \"/data/features_ring/\"\n",
        "path_pdb = current_directory + \"/data/pdb_files/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825
        },
        "id": "g9CpTg4Kuz53",
        "outputId": "590ce7c2-5612-4e7f-eb6d-6b06cab80ed5"
      },
      "outputs": [],
      "source": [
        "dfs = []\n",
        "for filename in os.listdir(path_ring):\n",
        "    dfs.append(pd.read_csv(path_ring + filename, sep='\\t'))\n",
        "df = pd.concat(dfs)\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "y = df['Interaction'].astype('category')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23o46QQlKUgz"
      },
      "source": [
        "# Add Feature: CA-CA Distances between source & target residues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "collapsed": true,
        "id": "LMWui1FIKRqX",
        "outputId": "02e4c776-79dc-45e2-85b2-622093894cae"
      },
      "outputs": [],
      "source": [
        "from Bio import PDB\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "\n",
        "parser = PDB.PDBParser(QUIET=True)\n",
        "\n",
        "\n",
        "def get_residue_distance(pdb_id, s_resi, t_resi, s_ch, t_ch, pdb_file_path):\n",
        "    \"\"\"Calculates CA-CA distance between two residues in a PDB file\"\"\"\n",
        "\n",
        "    structure = parser.get_structure(pdb_id, pdb_file_path)\n",
        "    model = structure[0]\n",
        "\n",
        "\n",
        "    try: # locate source- & target-chains\n",
        "        s_chain = model[s_ch]\n",
        "        t_chain = model[t_ch]\n",
        "    except KeyError:\n",
        "        raise ValueError(f\"Chain {s_ch} or {t_ch} not found in structure {pdb_id}\")\n",
        "\n",
        "\n",
        "    try: # locate source- & target-residues\n",
        "        s_residue = s_chain[s_resi]\n",
        "        t_residue = t_chain[t_resi]\n",
        "    except KeyError:\n",
        "        raise ValueError(f\"Residue {s_resi} or {t_resi} not found in chains {s_ch} or {t_ch}\")\n",
        "\n",
        "\n",
        "    try: # locate alpha carbons\n",
        "        s_ca = s_residue['CA']\n",
        "        t_ca = t_residue['CA']\n",
        "    except KeyError:\n",
        "        raise ValueError(f\"Alpha-carbon not found in residue {s_resi} or {t_resi}\")\n",
        "    \n",
        "\n",
        "    s_ca_coord = s_ca.get_coord()\n",
        "    t_ca_coord = t_ca.get_coord()\n",
        "\n",
        "    distance = np.linalg.norm(s_ca_coord - t_ca_coord)\n",
        "\n",
        "    return distance\n",
        "\n",
        "\n",
        "def process_row(index, row, pdb_directory):\n",
        "\n",
        "    pdb_id = row['pdb_id']\n",
        "    s_resi = row['s_resi']\n",
        "    t_resi = row['t_resi']\n",
        "    s_ch = row['s_ch']\n",
        "    t_ch = row['t_ch']\n",
        "\n",
        "    pdb_file_path = os.path.join(pdb_directory, f'{pdb_id}.pdb')\n",
        "\n",
        "    if not os.path.isfile(pdb_file_path):\n",
        "        print(f\"File {pdb_file_path} does not exist.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        distance = get_residue_distance(pdb_id, s_resi, t_resi, s_ch, t_ch, pdb_file_path)\n",
        "        return distance\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {pdb_id} (row {index}): {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def calculate_distances_parallel(df, pdb_directory, n_jobs=-1):\n",
        "    # Use Parallel to process each row in the dataframe in parallel\n",
        "    ca_distances = Parallel(n_jobs=n_jobs)(\n",
        "        delayed(process_row)(index, row, pdb_directory) for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing Rows\")\n",
        "    )\n",
        "    \n",
        "    # Add the result to the dataframe\n",
        "    df['CA_CA_distance'] = ca_distances\n",
        "    return df\n",
        "\n",
        "if not os.path.exists(current_directory + '/data/df_data.pkl'):\n",
        "    pdb_directory = path_pdb\n",
        "    df = calculate_distances_parallel(df, pdb_directory, n_jobs=-1)\n",
        "else:\n",
        "    df = pd.read_pickle(current_directory + '/data/df_data.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Add Feature: Sequence Neighbors (left/right) Aminoacid Type "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from Bio import PDB\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "import os\n",
        "\n",
        "def compute_residue_names(pdb_id, s_ch, s_resi, t_ch, t_resi, path_pdb):\n",
        "    pdb_file = path_pdb + f\"{pdb_id}.pdb\"\n",
        "    if not os.path.isfile(pdb_file):\n",
        "        return None, None, None, None\n",
        "    \n",
        "    structure = PDB.PDBParser(QUIET=True).get_structure(pdb_id, pdb_file)\n",
        "\n",
        "    s_resn_prev, s_resn_next, t_resn_prev, t_resn_next = None, None, None, None\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for residue in chain:\n",
        "                if chain.id == s_ch and residue.id[1] == s_resi - 1:\n",
        "                    s_resn_prev = residue.resname\n",
        "                if chain.id == s_ch and residue.id[1] == s_resi + 1:\n",
        "                    s_resn_next = residue.resname\n",
        "                if chain.id == t_ch and residue.id[1] == t_resi - 1:\n",
        "                    t_resn_prev = residue.resname\n",
        "                if chain.id == t_ch and residue.id[1] == t_resi + 1:\n",
        "                    t_resn_next = residue.resname\n",
        "\n",
        "    return s_resn_prev, s_resn_next, t_resn_prev, t_resn_next\n",
        "\n",
        "\n",
        "def process_row(row, path_pdb):\n",
        "    pdb_id = row['pdb_id']\n",
        "    s_ch = row['s_ch']\n",
        "    t_ch = row['t_ch']\n",
        "    s_resi = row['s_resi']\n",
        "    t_resi = row['t_resi']\n",
        "\n",
        "    return compute_residue_names(pdb_id, s_ch, s_resi, t_ch, t_resi, path_pdb)\n",
        "\n",
        "\n",
        "def process_dataset(df, path_pdb):\n",
        "\n",
        "    results = Parallel(n_jobs=-1)(\n",
        "        delayed(process_row)(row, path_pdb) for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Dataset\")\n",
        "    )\n",
        "\n",
        "    # Extract the results and add them as new columns\n",
        "    prev_s_resn_list, next_s_resn_list, prev_t_resn_list, next_t_resn_list = zip(*results)\n",
        "\n",
        "    df['prev_s_resn'] = prev_s_resn_list\n",
        "    df['next_s_resn'] = next_s_resn_list\n",
        "    df['prev_t_resn'] = prev_t_resn_list\n",
        "    df['next_t_resn'] = next_t_resn_list\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "if not os.path.exists(current_directory + '/data/df_data.pkl'):\n",
        "    df = process_dataset(df, path_pdb)\n",
        "else:\n",
        "    df = pd.read_pickle(current_directory + '/data/df_data.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Add Feature: Neighbors in 3D Space with sequence_separtion=6 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from Bio import PDB\n",
        "from tqdm import tqdm\n",
        "from Bio.PDB import PDBParser, NeighborSearch\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "three_to_one_letter = {\n",
        "    'ALA': 'A', 'ARG': 'R', 'ASN': 'N', 'ASP': 'D', 'CYS': 'C',\n",
        "    'GLU': 'E', 'GLN': 'Q', 'GLY': 'G', 'HIS': 'H', 'ILE': 'I',\n",
        "    'LEU': 'L', 'LYS': 'K', 'MET': 'M', 'PHE': 'F', 'PRO': 'P',\n",
        "    'SER': 'S', 'THR': 'T', 'TRP': 'W', 'TYR': 'Y', 'VAL': 'V'\n",
        "}\n",
        "\n",
        "parser = PDB.PDBParser(QUIET=True)\n",
        "\n",
        "def compute_residue_names(pdb_id, s_ch, s_resi, t_ch, t_resi):\n",
        "\n",
        "    pdb_file = os.path.join(path_pdb, f'{pdb_id}.pdb')\n",
        "    structure = parser.get_structure(pdb_id, pdb_file)\n",
        "    \n",
        "    source_residue = structure[0][s_ch][s_resi]\n",
        "    target_residue = structure[0][t_ch][t_resi]\n",
        "    \n",
        "    source_atoms = list(source_residue.get_atoms())\n",
        "    target_atoms = list(target_residue.get_atoms())\n",
        "\n",
        "    ns_source = NeighborSearch(source_atoms)\n",
        "    ns_target = NeighborSearch(target_atoms)\n",
        "\n",
        "    # Residues within 8.0 Å distance\n",
        "    contacts_source = ns_source.search_all(8.0, level=\"R\")\n",
        "    contacts_target = ns_target.search_all(8.0, level=\"R\")\n",
        "\n",
        "    # Combine contacts and remove duplicates\n",
        "    relevant_contacts = set(contacts_source) | set(contacts_target)\n",
        "\n",
        "    # Exclude contacts with sequence separation <= 6\n",
        "    filtered_contacts = []\n",
        "    for res1, res2 in relevant_contacts:\n",
        "        res1_id = res1.get_id()[1]  # Residue sequence number\n",
        "        res2_id = res2.get_id()[1]  # Residue sequence number\n",
        "\n",
        "        if abs(res1_id - res2_id) > 6:\n",
        "            filtered_contacts.append((res1, res2))\n",
        "\n",
        "    # Extract coordinates\n",
        "    source_coords = np.array([atom.coord for atom in source_atoms])\n",
        "    target_coords = np.array([atom.coord for atom in target_atoms])\n",
        "\n",
        "    min_distance_s, min_distance_t = float('inf'), float('inf')\n",
        "    s_resn_neighbour, t_resn_neighbour = None, None\n",
        "\n",
        "    # Process filtered contacts to find the closest residues\n",
        "    for residue1, residue2 in filtered_contacts:\n",
        "        if residue1 == target_residue or residue2 == target_residue:\n",
        "            other_residue = residue2 if residue1 == target_residue else residue1\n",
        "            other_coords = np.array([atom.coord for atom in other_residue.get_atoms()])\n",
        "            distances = np.linalg.norm(target_coords[:, np.newaxis] - other_coords, axis=-1)\n",
        "            min_distance = np.min(distances)\n",
        "\n",
        "            if min_distance < min_distance_t:\n",
        "                min_distance_t = min_distance\n",
        "                t_resn_neighbour = other_residue\n",
        "\n",
        "        if residue1 == source_residue or residue2 == source_residue:\n",
        "            other_residue = residue2 if residue1 == source_residue else residue1\n",
        "            other_coords = np.array([atom.coord for atom in other_residue.get_atoms()])\n",
        "            distances = np.linalg.norm(source_coords[:, np.newaxis] - other_coords, axis=-1)\n",
        "            min_distance = np.min(distances)\n",
        "\n",
        "            if min_distance < min_distance_s:\n",
        "                min_distance_s = min_distance\n",
        "                s_resn_neighbour = other_residue\n",
        "\n",
        "    return (s_resn_neighbour.get_resname() if s_resn_neighbour else None,\n",
        "            t_resn_neighbour.get_resname() if t_resn_neighbour else None)\n",
        "\n",
        "\n",
        "def process_row(row):\n",
        "    pdb_id = row['pdb_id']\n",
        "    s_ch = row['s_ch']\n",
        "    t_ch = row['t_ch']\n",
        "    s_resi = row['s_resi']\n",
        "    t_resi = row['t_resi']\n",
        "\n",
        "    s_resn_neighbour, t_resn_neighbour = compute_residue_names(pdb_id, s_ch, s_resi, t_ch, t_resi)\n",
        "    \n",
        "    s_resn_neighbour = three_to_one_letter.get(s_resn_neighbour, s_resn_neighbour)\n",
        "    t_resn_neighbour = three_to_one_letter.get(t_resn_neighbour, t_resn_neighbour)\n",
        "    \n",
        "    return s_resn_neighbour, t_resn_neighbour\n",
        "\n",
        "\n",
        "def process_dataset(df, n_jobs=-1):\n",
        "    results = Parallel(n_jobs=n_jobs)(\n",
        "        delayed(process_row)(row) for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing rows\")\n",
        "    )\n",
        "\n",
        "    s_resn_neighbours_list, t_resn_neighbours_list = zip(*results)\n",
        "    df['s_resn_neighbour'] = s_resn_neighbours_list\n",
        "    df['t_resn_neighbour'] = t_resn_neighbours_list\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "if not os.path.exists(current_directory + '/data/df_data.pkl'):\n",
        "    df = process_dataset(df, path_pdb)\n",
        "else:\n",
        "    df = pd.read_pickle(current_directory + '/data/df_data.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idIoTxLVRjnH"
      },
      "source": [
        "# Add Feature: Calculate Electrostatic Interaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOVOr5t8RoA-",
        "outputId": "4c59b54f-e063-4476-94c6-01055a3b7a13"
      },
      "outputs": [],
      "source": [
        "one_to_three_letter = {\n",
        "    'A': 'ALA', 'R': 'ARG', 'N': 'ASN', 'D': 'ASP', 'C': 'CYS',\n",
        "    'E': 'GLU', 'Q': 'GLN', 'G': 'GLY', 'H': 'HIS', 'I': 'ILE',\n",
        "    'L': 'LEU', 'K': 'LYS', 'M': 'MET', 'F': 'PHE', 'P': 'PRO',\n",
        "    'S': 'SER', 'T': 'THR', 'W': 'TRP', 'Y': 'TYR', 'V': 'VAL'\n",
        "}\n",
        "\n",
        "\n",
        "def get_residue_charge(resn):\n",
        "    charges = {'ASP': -1, 'GLU': -1, 'LYS': 1, 'ARG': 1, 'HIS': 1}\n",
        "    return charges.get(resn, 0)  # Default to 0 for neutral residues\n",
        "\n",
        "\n",
        "def compute_electrostatic_energy(q1, q2, r, epsilon=4.0):\n",
        "    \"\"\"Calculates electrostatic energy using Coulomb's Law\"\"\"\n",
        "    k_e = 8.9875 * 10**9  # Coulomb constant in N m^2 C^-2\n",
        "    return (k_e * q1 * q2) / (epsilon * r)\n",
        "\n",
        "\n",
        "def compute_interaction(s_resn, t_resn, distance):\n",
        "    \"\"\"Computes electrostatic interaction energy using separate parameters\"\"\"\n",
        "\n",
        "    s_resn_three = one_to_three_letter.get(s_resn, None)\n",
        "    t_resn_three = one_to_three_letter.get(t_resn, None)\n",
        "\n",
        "    if s_resn_three is None or t_resn_three is None:\n",
        "        print(f\"Invalid residue code: {s_resn} or {t_resn}\")\n",
        "        return None\n",
        "\n",
        "    q1 = get_residue_charge(s_resn_three)\n",
        "    q2 = get_residue_charge(t_resn_three)\n",
        "\n",
        "    if distance is None or distance == 0:\n",
        "        print(f\"Distance error for residue pair: {s_resn} - {t_resn}\")\n",
        "        return None\n",
        "\n",
        "    return compute_electrostatic_energy(q1, q2, distance)\n",
        "\n",
        "computed_energies = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    distance = row['CA_CA_distance']\n",
        "    energy = compute_interaction(row['s_resn'], row['t_resn'], distance)\n",
        "    computed_energies.append(energy)\n",
        "df['electrostatic_energy'] = computed_energies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsaIajMHMEyM",
        "outputId": "fbac0f4a-5d4b-4560-cea5-5a59c065e279"
      },
      "outputs": [],
      "source": [
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "def encode_object_columns(df):\n",
        "    label_encoder = LabelEncoder()\n",
        "    for column in df.columns:\n",
        "        if df[column].dtype == 'object':\n",
        "            df[column] = df[column].astype(str)\n",
        "            df[column] = label_encoder.fit_transform(df[column])\n",
        "    return df\n",
        "\n",
        "df = encode_object_columns(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPYNNbasSyEO"
      },
      "outputs": [],
      "source": [
        "X = df.drop(columns=['Interaction'])\n",
        "y = df['Interaction']\n",
        "\n",
        "y = to_categorical(y, num_classes=10)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        },
        "id": "J-ItyEjPTo8P",
        "outputId": "520f8971-7104-4f11-92b9-d09443bf9353"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample a subset of the data\n",
        "sample_size = 50000\n",
        "df_sample = df.sample(n=sample_size, random_state=42)\n",
        "\n",
        "X_sample = df_sample.drop(columns=['Interaction'])\n",
        "y_sample = df_sample['Interaction']\n",
        "\n",
        "# Convert labels to one-hot encoding if necessary and then back to labels for Random Forest\n",
        "y_sample = pd.get_dummies(y_sample)  # One-hot encode if needed\n",
        "y_sample_labels = y_sample.values.argmax(axis=1)  # Convert back to labels\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=50, max_depth=20, random_state=42)  # Reduced parameters\n",
        "rf.fit(X_sample, y_sample_labels)\n",
        "\n",
        "importances = rf.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title(\"Feature Importance\")\n",
        "plt.bar(range(X_sample.shape[1]), importances[indices], align=\"center\")\n",
        "plt.xticks(range(X_sample.shape[1]), X_sample.columns[indices], rotation=90)\n",
        "plt.xlim([-1, X_sample.shape[1]])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9j4aflZcSgjn"
      },
      "outputs": [],
      "source": [
        "X = df.drop(columns=['Interaction', 't_ss3', \"s_ss3\", \"t_ins\", \"s_ins\" ])\n",
        "y = df['Interaction']\n",
        "\n",
        "y = to_categorical(y, num_classes=10) # One-hot encode the labels\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Handle Imbalanced Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBObwIc6DYc_"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "y_train_labels = np.argmax(y_train, axis=1)\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_labels), y=y_train_labels)\n",
        "class_weight_dict = dict(enumerate(class_weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "A-SqBliFJLjv",
        "outputId": "0ef7559e-91ff-4de2-9b38-0d2fd483ed3c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Input layer and first hidden layer\n",
        "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Second hidden layer\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Third hidden layer\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Fourth hidden layer\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "optimizer = Adam(learning_rate=0.0001)\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Implement early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Define K-Fold cross-validator\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Store scores for each fold\n",
        "fold_accuracies = []\n",
        "\n",
        "# K-Fold Cross Validation\n",
        "for train_index, val_index in kf.split(X_train):\n",
        "    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
        "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "\n",
        "    # Create a new model instance for each fold\n",
        "    optimizer = Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Early stopping callback\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train_fold, y_train_fold,\n",
        "                        validation_data=(X_val_fold, y_val_fold),\n",
        "                        epochs=50,\n",
        "                        batch_size=32,\n",
        "                        class_weight=class_weight_dict,\n",
        "                        callbacks=[early_stopping],\n",
        "                        verbose=0)  # Set verbose to 1 for detailed output\n",
        "\n",
        "   # Predict the labels for the validation fold\n",
        "    y_val_pred = np.argmax(model.predict(X_val_fold), axis=1)\n",
        "    y_val_true = np.argmax(y_val_fold, axis=1)\n",
        "\n",
        "    # Compute balanced accuracy for the current fold\n",
        "    balanced_acc = balanced_accuracy_score(y_val_true, y_val_pred)\n",
        "    print(f\"Balanced accuracy for current fold: {balanced_acc:.4f}\")\n",
        "\n",
        "    fold_accuracies.append(balanced_acc)\n",
        "\n",
        "# Calculate and print the average balanced accuracy across all folds\n",
        "average_balanced_accuracy = np.mean(fold_accuracies)\n",
        "print(f\"Average balanced accuracy across all folds: {average_balanced_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToCjxj1tJ9nK",
        "outputId": "b4a06c53-98fa-4014-b335-95abc6ab182e"
      },
      "outputs": [],
      "source": [
        "y_pred_proba = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "mcc = matthews_corrcoef(y_true, y_pred)\n",
        "balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
        "avg_precision = average_precision_score(y_test, y_pred_proba, average='macro')\n",
        "\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n",
        "print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
        "print(f\"Average Precision Score: {avg_precision:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
