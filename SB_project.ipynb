{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiAHkyDdV8-t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "from biopandas.pdb import PandasPdb\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from Bio.PDB import DSSP, HSExposureCB, PPBuilder, is_aa, NeighborSearch\n",
        "from Bio.PDB.MMCIFParser import MMCIFParser\n",
        "from Bio.SeqUtils import seq1\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, CategoricalNB, ComplementNB, BernoulliNB\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from timeit import default_timer as timer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import matthews_corrcoef, balanced_accuracy_score, average_precision_score, roc_auc_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nWqgNFSup2mP"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "current_directory = os.getcwd()\n",
        "path_ring = current_directory + \"/data/features_ring/\"\n",
        "path_pdb = current_directory + \"/data/pdb_files/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825
        },
        "id": "g9CpTg4Kuz53",
        "outputId": "590ce7c2-5612-4e7f-eb6d-6b06cab80ed5"
      },
      "outputs": [],
      "source": [
        "dfs = []\n",
        "for filename in os.listdir(path_ring):\n",
        "    dfs.append(pd.read_csv(path_ring + filename, sep='\\t'))\n",
        "df = pd.concat(dfs)\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "y = df['Interaction'].astype('category')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23o46QQlKUgz"
      },
      "source": [
        "# Add Feature: CA-CA Distances between source & target residues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "collapsed": true,
        "id": "LMWui1FIKRqX",
        "outputId": "02e4c776-79dc-45e2-85b2-622093894cae"
      },
      "outputs": [],
      "source": [
        "from Bio import PDB\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "\n",
        "parser = PDB.PDBParser(QUIET=True)\n",
        "\n",
        "\n",
        "def get_residue_distance(pdb_id, s_resi, t_resi, s_ch, t_ch, pdb_file_path):\n",
        "    \"\"\"Calculates CA-CA distance between two residues in a PDB file\"\"\"\n",
        "\n",
        "    structure = parser.get_structure(pdb_id, pdb_file_path)\n",
        "    model = structure[0]\n",
        "\n",
        "\n",
        "    try: # locate source- & target-chains\n",
        "        s_chain = model[s_ch]\n",
        "        t_chain = model[t_ch]\n",
        "    except KeyError:\n",
        "        raise ValueError(f\"Chain {s_ch} or {t_ch} not found in structure {pdb_id}\")\n",
        "\n",
        "\n",
        "    try: # locate source- & target-residues\n",
        "        s_residue = s_chain[s_resi]\n",
        "        t_residue = t_chain[t_resi]\n",
        "    except KeyError:\n",
        "        raise ValueError(f\"Residue {s_resi} or {t_resi} not found in chains {s_ch} or {t_ch}\")\n",
        "\n",
        "\n",
        "    try: # locate alpha carbons\n",
        "        s_ca = s_residue['CA']\n",
        "        t_ca = t_residue['CA']\n",
        "    except KeyError:\n",
        "        raise ValueError(f\"Alpha-carbon not found in residue {s_resi} or {t_resi}\")\n",
        "    \n",
        "\n",
        "    s_ca_coord = s_ca.get_coord()\n",
        "    t_ca_coord = t_ca.get_coord()\n",
        "\n",
        "    distance = np.linalg.norm(s_ca_coord - t_ca_coord)\n",
        "\n",
        "    return distance\n",
        "\n",
        "\n",
        "def process_row(index, row, pdb_directory):\n",
        "\n",
        "    pdb_id = row['pdb_id']\n",
        "    s_resi = row['s_resi']\n",
        "    t_resi = row['t_resi']\n",
        "    s_ch = row['s_ch']\n",
        "    t_ch = row['t_ch']\n",
        "\n",
        "    pdb_file_path = os.path.join(pdb_directory, f'{pdb_id}.pdb')\n",
        "\n",
        "    if not os.path.isfile(pdb_file_path):\n",
        "        print(f\"File {pdb_file_path} does not exist.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        distance = get_residue_distance(pdb_id, s_resi, t_resi, s_ch, t_ch, pdb_file_path)\n",
        "        return distance\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {pdb_id} (row {index}): {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def calculate_distances_parallel(df, pdb_directory, n_jobs=-1):\n",
        "    # Use Parallel to process each row in the dataframe in parallel\n",
        "    ca_distances = Parallel(n_jobs=n_jobs)(\n",
        "        delayed(process_row)(index, row, pdb_directory) for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing Rows\")\n",
        "    )\n",
        "    \n",
        "    # Add the result to the dataframe\n",
        "    df['CA_CA_distance'] = ca_distances\n",
        "    return df\n",
        "\n",
        "if not os.path.exists(current_directory + '/data/df_data.pkl'):\n",
        "    pdb_directory = path_pdb\n",
        "    df = calculate_distances_parallel(df, pdb_directory, n_jobs=-1)\n",
        "else:\n",
        "    df = pd.read_pickle(current_directory + '/data/df_data.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Add Feature: Sequence Neighbors (left/right) Aminoacid Type "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from Bio import PDB\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "import os\n",
        "\n",
        "def compute_residue_names(pdb_id, s_ch, s_resi, t_ch, t_resi, path_pdb):\n",
        "    pdb_file = path_pdb + f\"{pdb_id}.pdb\"\n",
        "    if not os.path.isfile(pdb_file):\n",
        "        return None, None, None, None\n",
        "    \n",
        "    structure = PDB.PDBParser(QUIET=True).get_structure(pdb_id, pdb_file)\n",
        "\n",
        "    s_resn_prev, s_resn_next, t_resn_prev, t_resn_next = None, None, None, None\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for residue in chain:\n",
        "                if chain.id == s_ch and residue.id[1] == s_resi - 1:\n",
        "                    s_resn_prev = residue.resname\n",
        "                if chain.id == s_ch and residue.id[1] == s_resi + 1:\n",
        "                    s_resn_next = residue.resname\n",
        "                if chain.id == t_ch and residue.id[1] == t_resi - 1:\n",
        "                    t_resn_prev = residue.resname\n",
        "                if chain.id == t_ch and residue.id[1] == t_resi + 1:\n",
        "                    t_resn_next = residue.resname\n",
        "\n",
        "    return s_resn_prev, s_resn_next, t_resn_prev, t_resn_next\n",
        "\n",
        "\n",
        "def process_row(row, path_pdb):\n",
        "    pdb_id = row['pdb_id']\n",
        "    s_ch = row['s_ch']\n",
        "    t_ch = row['t_ch']\n",
        "    s_resi = row['s_resi']\n",
        "    t_resi = row['t_resi']\n",
        "\n",
        "    return compute_residue_names(pdb_id, s_ch, s_resi, t_ch, t_resi, path_pdb)\n",
        "\n",
        "\n",
        "def process_dataset(df, path_pdb):\n",
        "\n",
        "    results = Parallel(n_jobs=-1)(\n",
        "        delayed(process_row)(row, path_pdb) for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Dataset\")\n",
        "    )\n",
        "\n",
        "    # Extract the results and add them as new columns\n",
        "    prev_s_resn_list, next_s_resn_list, prev_t_resn_list, next_t_resn_list = zip(*results)\n",
        "\n",
        "    df['prev_s_resn'] = prev_s_resn_list\n",
        "    df['next_s_resn'] = next_s_resn_list\n",
        "    df['prev_t_resn'] = prev_t_resn_list\n",
        "    df['next_t_resn'] = next_t_resn_list\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "if not os.path.exists(current_directory + '/data/df_data.pkl'):\n",
        "    df = process_dataset(df, path_pdb)\n",
        "else:\n",
        "    df = pd.read_pickle(current_directory + '/data/df_data.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Add Feature: Neighbors in 3D Space with sequence_separtion=6 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from Bio import PDB\n",
        "from tqdm import tqdm\n",
        "from Bio.PDB import PDBParser, NeighborSearch\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "three_to_one_letter = {\n",
        "    'ALA': 'A', 'ARG': 'R', 'ASN': 'N', 'ASP': 'D', 'CYS': 'C',\n",
        "    'GLU': 'E', 'GLN': 'Q', 'GLY': 'G', 'HIS': 'H', 'ILE': 'I',\n",
        "    'LEU': 'L', 'LYS': 'K', 'MET': 'M', 'PHE': 'F', 'PRO': 'P',\n",
        "    'SER': 'S', 'THR': 'T', 'TRP': 'W', 'TYR': 'Y', 'VAL': 'V'\n",
        "}\n",
        "\n",
        "parser = PDB.PDBParser(QUIET=True)\n",
        "\n",
        "def compute_residue_names(pdb_id, s_ch, s_resi, t_ch, t_resi):\n",
        "\n",
        "    pdb_file = os.path.join(path_pdb, f'{pdb_id}.pdb')\n",
        "    structure = parser.get_structure(pdb_id, pdb_file)\n",
        "    \n",
        "    source_residue = structure[0][s_ch][s_resi]\n",
        "    target_residue = structure[0][t_ch][t_resi]\n",
        "\n",
        "    all_atoms = list(structure.get_atoms())  # Search across the entire structure\n",
        "    ns = NeighborSearch(all_atoms)\n",
        "\n",
        "    # Residues within 8.0 Å distance\n",
        "    contacts = ns.search_all(8.0, level=\"R\")\n",
        "\n",
        "    # Exclude contacts with sequence separation <= 6\n",
        "    filtered_contacts = []\n",
        "    for res1, res2 in contacts:\n",
        "\n",
        "        is_amino_acid_1 = is_aa(res1)\n",
        "        is_amino_acid_2 = is_aa(res2)\n",
        "\n",
        "        if is_amino_acid_1 and is_amino_acid_2:\n",
        "            res1_id = res1.get_id()[1]\n",
        "            res2_id = res2.get_id()[1]\n",
        "\n",
        "            if abs(res1_id - res2_id) > 6:\n",
        "                filtered_contacts.append((res1, res2))\n",
        "\n",
        "    # Extract coordinates\n",
        "    source_coords = np.array([atom.coord for atom in source_residue.get_atoms()])\n",
        "    target_coords = np.array([atom.coord for atom in target_residue.get_atoms()])\n",
        "\n",
        "    min_distance_s, min_distance_t = float('inf'), float('inf')\n",
        "    s_resn_neighbour, t_resn_neighbour = None, None\n",
        "\n",
        "    # Process filtered contacts to find the closest residues\n",
        "    for residue1, residue2 in filtered_contacts:\n",
        "        if residue1 == target_residue or residue2 == target_residue:\n",
        "            other_residue = residue2 if residue1 == target_residue else residue1\n",
        "            other_coords = np.array([atom.coord for atom in other_residue.get_atoms()])\n",
        "            distances = np.linalg.norm(target_coords[:, np.newaxis] - other_coords, axis=-1)\n",
        "            min_distance = np.min(distances)\n",
        "\n",
        "            if min_distance < min_distance_t:\n",
        "                min_distance_t = min_distance\n",
        "                t_resn_neighbour = other_residue\n",
        "\n",
        "        if residue1 == source_residue or residue2 == source_residue:\n",
        "            other_residue = residue2 if residue1 == source_residue else residue1\n",
        "            other_coords = np.array([atom.coord for atom in other_residue.get_atoms()])\n",
        "            distances = np.linalg.norm(source_coords[:, np.newaxis] - other_coords, axis=-1)\n",
        "            min_distance = np.min(distances)\n",
        "\n",
        "            if min_distance < min_distance_s:\n",
        "                min_distance_s = min_distance\n",
        "                s_resn_neighbour = other_residue\n",
        "\n",
        "    return (s_resn_neighbour.get_resname(),\n",
        "            t_resn_neighbour.get_resname())\n",
        "\n",
        "\n",
        "def process_row(row):\n",
        "    pdb_id = row['pdb_id']\n",
        "    s_ch = row['s_ch']\n",
        "    t_ch = row['t_ch']\n",
        "    s_resi = row['s_resi']\n",
        "    t_resi = row['t_resi']\n",
        "\n",
        "    s_resn_neighbour, t_resn_neighbour = compute_residue_names(pdb_id, s_ch, s_resi, t_ch, t_resi)\n",
        "    \n",
        "    s_resn_neighbour = three_to_one_letter.get(s_resn_neighbour, s_resn_neighbour)\n",
        "    t_resn_neighbour = three_to_one_letter.get(t_resn_neighbour, t_resn_neighbour)\n",
        "    \n",
        "    return s_resn_neighbour, t_resn_neighbour\n",
        "\n",
        "\n",
        "def process_dataset(df, n_jobs=-1):\n",
        "    results = Parallel(n_jobs=n_jobs)(\n",
        "        delayed(process_row)(row) for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing rows\")\n",
        "    )\n",
        "\n",
        "    s_resn_neighbours_list, t_resn_neighbours_list = zip(*results)\n",
        "    df['s_resn_neighbour'] = s_resn_neighbours_list\n",
        "    df['t_resn_neighbour'] = t_resn_neighbours_list\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "df = process_dataset(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsaIajMHMEyM",
        "outputId": "fbac0f4a-5d4b-4560-cea5-5a59c065e279"
      },
      "outputs": [],
      "source": [
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "def encode_object_columns(df):\n",
        "    label_encoder = LabelEncoder()\n",
        "    for column in df.columns:\n",
        "        if df[column].dtype == 'object':\n",
        "            df[column] = df[column].astype(str)\n",
        "            df[column] = label_encoder.fit_transform(df[column])\n",
        "    return df\n",
        "\n",
        "df = encode_object_columns(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPYNNbasSyEO"
      },
      "outputs": [],
      "source": [
        "X = df.drop(columns=['Interaction'])\n",
        "y = df['Interaction']\n",
        "\n",
        "y = to_categorical(y, num_classes=10)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        },
        "id": "J-ItyEjPTo8P",
        "outputId": "520f8971-7104-4f11-92b9-d09443bf9353"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample a subset of the data\n",
        "sample_size = 50000\n",
        "df_sample = df.sample(n=sample_size, random_state=42)\n",
        "\n",
        "X_sample = df_sample.drop(columns=['Interaction'])\n",
        "y_sample = df_sample['Interaction']\n",
        "\n",
        "# Convert labels to one-hot encoding if necessary and then back to labels for Random Forest\n",
        "y_sample = pd.get_dummies(y_sample)  # One-hot encode if needed\n",
        "y_sample_labels = y_sample.values.argmax(axis=1)  # Convert back to labels\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=50, max_depth=20, random_state=42)  # Reduced parameters\n",
        "rf.fit(X_sample, y_sample_labels)\n",
        "\n",
        "importances = rf.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title(\"Feature Importance\")\n",
        "plt.bar(range(X_sample.shape[1]), importances[indices], align=\"center\")\n",
        "plt.xticks(range(X_sample.shape[1]), X_sample.columns[indices], rotation=90)\n",
        "plt.xlim([-1, X_sample.shape[1]])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9j4aflZcSgjn"
      },
      "outputs": [],
      "source": [
        "X = df.drop(columns=['Interaction', 't_ss3', \"s_ss3\", \"t_ins\", \"s_ins\" ])\n",
        "y = df['Interaction']\n",
        "\n",
        "y = to_categorical(y, num_classes=10) # One-hot encode the labels\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Handle Imbalanced Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBObwIc6DYc_"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "y_train_labels = np.argmax(y_train, axis=1)\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_labels), y=y_train_labels)\n",
        "class_weight_dict = dict(enumerate(class_weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "A-SqBliFJLjv",
        "outputId": "0ef7559e-91ff-4de2-9b38-0d2fd483ed3c"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import ADASYN\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Input layer and first hidden layer\n",
        "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Second hidden layer\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Third hidden layer\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Fourth hidden layer\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "optimizer = Adam(learning_rate=0.0001)\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Implement early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Define K-Fold cross-validator\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Store scores for each fold\n",
        "fold_accuracies = []\n",
        "per_class_accuracies = []\n",
        "\n",
        "# ADASYN for balancing the data\n",
        "adasyn = ADASYN()\n",
        "\n",
        "# K-Fold Cross Validation\n",
        "for train_index, val_index in tqdm(kf.split(X_train), total=kf.get_n_splits(), desc=\"Cross-Validation\"):\n",
        "    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
        "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "        \n",
        "    X_train_fold_resampled, y_train_fold_resampled = adasyn.fit_resample(X_train_fold, y_train_fold)\n",
        "\n",
        "    # Create a new model instance for each fold\n",
        "    optimizer = Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Early stopping callback\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train_fold_resampled, y_train_fold_resampled,\n",
        "                        validation_data=(X_val_fold, y_val_fold),\n",
        "                        epochs=50,\n",
        "                        batch_size=32,\n",
        "                        class_weight=class_weight_dict,\n",
        "                        callbacks=[early_stopping],\n",
        "                        verbose=0)  # Set verbose to 1 for detailed output\n",
        "\n",
        "   # Predict the labels for the validation fold\n",
        "    y_val_pred = np.argmax(model.predict(X_val_fold), axis=1)\n",
        "    y_val_true = np.argmax(y_val_fold, axis=1)\n",
        "\n",
        "    # Compute balanced accuracy for the current fold\n",
        "    balanced_acc = balanced_accuracy_score(y_val_true, y_val_pred)\n",
        "    print(f\"Balanced accuracy for current fold: {balanced_acc:.4f}\")\n",
        "\n",
        "    fold_accuracies.append(balanced_acc)\n",
        "\n",
        "    # Calculate and print per-class accuracy\n",
        "    class_report = classification_report(y_val_true, y_val_pred, output_dict=True)\n",
        "    per_class_acc = {f\"Class {cls}\": class_report[str(cls)]['precision'] for cls in range(7)}\n",
        "    print(f\"Per-class accuracy for current fold: {per_class_acc}\")\n",
        "\n",
        "    per_class_accuracies.append(per_class_acc)\n",
        "\n",
        "# Calculate and print the average balanced accuracy across all folds\n",
        "average_balanced_accuracy = np.mean(fold_accuracies)\n",
        "print(f\"Average balanced accuracy across all folds: {average_balanced_accuracy:.4f}\")\n",
        "\n",
        "# Calculate average per-class accuracy across all folds\n",
        "average_per_class_acc = {f\"Class {cls}\": np.mean([acc[f\"Class {cls}\"] for acc in per_class_accuracies]) for cls in range(7)}\n",
        "print(f\"Average per-class accuracy across all folds: {average_per_class_acc}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "# Save the fold accuracies and per-class accuracies to a CSV file\n",
        "with open('kfold_results.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    \n",
        "    # Write headers\n",
        "    writer.writerow(['Fold', 'Balanced Accuracy'] + [f'Class {i} Accuracy' for i in range(7)])\n",
        "    \n",
        "    # Write data for each fold\n",
        "    for i, (fold_acc, class_acc) in enumerate(zip(fold_accuracies, per_class_accuracies)):\n",
        "        writer.writerow([i + 1, fold_acc] + [class_acc.get(f'Class {cls}', 0) for cls in range(7)])\n",
        "    \n",
        "    # Write average accuracies\n",
        "    writer.writerow(['Average', average_balanced_accuracy] + [average_per_class_acc.get(f'Class {cls}', 0) for cls in range(7)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToCjxj1tJ9nK",
        "outputId": "b4a06c53-98fa-4014-b335-95abc6ab182e"
      },
      "outputs": [],
      "source": [
        "y_pred_proba = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "mcc = matthews_corrcoef(y_true, y_pred)\n",
        "balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
        "avg_precision = average_precision_score(y_test, y_pred_proba, average='macro')\n",
        "\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n",
        "print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
        "print(f\"Average Precision Score: {avg_precision:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
